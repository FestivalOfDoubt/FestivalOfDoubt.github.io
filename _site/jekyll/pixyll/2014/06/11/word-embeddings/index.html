<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Semantic Word Embeddings &#8211; Off the convex path</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Understanding word embeddings">
    <meta name="author" content="Moritz Hardt">
    <meta name="keywords" content="jekyll, pixyll">
    <link rel="canonical" href="http://offconvex.github.io/jekyll/pixyll/2014/06/11/word-embeddings/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for Off the convex path" href="/feed.xml" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/pixyll.css?201511242032" type="text/css">

    <!-- Fonts -->
    <link href='//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:900,300' rel='stylesheet' type='text/css'>
    
      <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css" rel="stylesheet">
    

    <!-- Verifications -->
    

    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_US">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Semantic Word Embeddings">
    <meta property="og:description" content="Algorithms off the convex path.">
    <meta property="og:url" content="http://offconvex.github.io/jekyll/pixyll/2014/06/11/word-embeddings/">
    <meta property="og:site_name" content="Off the convex path">

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary" />
    
    <meta name="twitter:title" content="Semantic Word Embeddings" />
    <meta name="twitter:description" content="Understanding word embeddings" />
    <meta name="twitter:url" content="http://offconvex.github.io/jekyll/pixyll/2014/06/11/word-embeddings/" />

    <!-- Icons -->
    <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">
    <link rel="icon" type="image/png" href="/favicon-192x192.png" sizes="192x192">
    <link rel="icon" type="image/png" href="/favicon-160x160.png" sizes="160x160">
    <link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">

    <script type="text/javascript"
     src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-70478681-1', 'auto');
  ga('send', 'pageview');

</script>
</head>

<body class="site">

	

  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      <a href="http://offconvex.github.io" class="site-title">
      <img style="width:600px;" src="/assets/logo.png" />
      </a>
      <nav class="site-nav" style="padding-top:230px;">
        <a href="/about/">About</a>
<a href="/contact/">Contact</a>

      </nav>
      <div class="clearfix"></div>
      
    </div>
  </div>
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">
        


<div class="post-header mb2">
  <h1>Semantic Word Embeddings</h1>
  <span class="post-meta">Jun 11, 2014</span><br>
  
  <span class="post-meta small">
  
    5 minute read
  
  </span>
</div>

<article class="post-content">
  <p>This post can be seen as an introduction to why nonconvex problems arise
naturally in practice, and also the relative ease by which they are often
solved.</p>

<p>The topic is <em>word embeddings</em>, a geometric way to capture
the “meaning” of a word via a low-dimensional vector. They are useful in
many tasks in Information Retrieval (IR) and Natural Language Processing
(NLP), for example answering search queries or translating from one
language to another.</p>

<p>You may wonder: how can a 300-dimensional vector capture the many
nuances of word meaning? And what the heck does this mean?</p>

<p>A simple property of embeddings obtained by all the methods I’ll
describe is <em>cosine similarity</em>: the  <em>similarity</em> between two words 
(as rated by humans on a <script type="math/tex">[-1,1]</script> scale) correlates with the <em>cosine</em>
of the angle between their vectors. To 
give an example, the cosine for <em>milk</em> and
<em>cow</em> may be <script type="math/tex">0.7</script>, whereas for <em>milk</em> and
<em>stone</em> it may be <script type="math/tex">-0.1</script>, which is roughly the similarity
human subjects assign to them.</p>

<p>A more interesting property in recent embeddings is that they can solve
<em>analogy</em> relationships via linear algebra on embeddings.
For example, the word analogy question
<em>man : woman ::king : ??</em> can be solved by looking for the
word <script type="math/tex">w</script> such that <script type="math/tex">v_{king} - v_w</script> is most similar to
<script type="math/tex">v_{man} - v_{woman}</script>; in other words, minimizes</p>

<script type="math/tex; mode=display">||v_w - v_{king} + v_{man} - v_{woman}||^2.</script>

<p>This simple idea can solve <script type="math/tex">75\%</script> of analogy questions on some standard testbed.</p>

<p><img src="/assets/linearrelations.jpg" alt="linear" /></p>

<p>Good embeddings have other properties that will be covered in a future
post. Now let’s discuss simple methods to construct them.</p>

<p>The methods all ultimately rely on <em>Firth’s hypothesis</em>
from 1954, which says that Namely, the meaning of a word is determined
by the distribution of other words around it. To give an example, if I
ask you to think of a word that tends to co-occur with <em>cow,
drink, babies, calcium</em>, you would immediately answer:
<em>milk</em>.</p>

<p>Note that I am not suggesting that Firth’s hypothesis fully accounts
for all aspects of semantics. (If it it did, a computer would be able to
learn language completely in an unsupervised way by processing a large
enough text corpus. Many experts doubt this is possible: they feel
language –e.g. understanding new metaphors or jokes— requires some
knowledge of the physical world, as well as some $2$-way interaction
between teacher and student.) But it does imply a very simple 
word embedding, albeit a very high-dimensional one.</p>

<blockquote>
  <p><em>Embedding 1</em>:</p>

  <p>Suppose the dictionary has <script type="math/tex">N</script> distinct words (in practice, <script type="math/tex">N =100,000</script>). Take a very large text corpus (e.g., Wikipedia) and let <script type="math/tex">Count_5(w_1, w_2)</script> be the number of times <script type="math/tex">w_1</script> and <script type="math/tex">w_2</script> occur within a distance <script type="math/tex">5</script> of each other in the corpus. Then the word embedding for a word <script type="math/tex">w</script> is a vector of dimension <script type="math/tex">N</script>, with one coordinate for each dictionary word. The coordinate corresponding to word <script type="math/tex">w_2</script> is <script type="math/tex">Count_5(w, w_2)</script>.</p>
</blockquote>

<p>The obvious problem with Embedding 1 is that it uses
extremely high-dimensional vectors. How can we compress them?</p>

<blockquote>
  <p><em>Embedding 2</em>: Do dimension reduction by taking the rank-<script type="math/tex">300</script>
singular value decomposition (SVD) of the above vectors.</p>
</blockquote>

<p>Recall that for an <script type="math/tex">N \times N</script> matrix <script type="math/tex">M</script> this means finding vectors
<script type="math/tex">v_1, v_2, \ldots, v_N 
\in \mathbb{R}^{300}</script> that minimize</p>

<script type="math/tex; mode=display">\sum_{ij} (M_{ij} - v_i \cdot v_j)^2 \qquad (1).</script>

<p>Using SVD to do dimension reduction seems an obvious idea today but it
actually is not since it is unclear <em>a priori</em> why the
above <script type="math/tex">N \times N</script> matrix of cooccurance counts should be close to a
rank-300 matrix. This was empirically discovered in the paper on
<em>Latent Semantic Analysis</em> or LSA. (It shows in fact that
low-dimensional embeddings are <em>better</em> than
high-dimensional ones.)</p>

<p>A research area called <em>Vector Space Models</em> (see survey by
Turney and Pantel) studies various modifications of the above idea, most
of which involve reweighting the above raw counts: some buzzwords are
TF-IDF, PMI, Logarithm, Square-root, etc. In general reweighting the
<script type="math/tex">(i, j)</script> term in expression (1) leads to a <em>weighted</em>
version of SVD, which is NP-hard. (I always emphasize to my students
what a miraculous algorithm SVD is, since modifying the problem
statement in small ways makes it NP-hard.) But in practice, it can be
solved by simple gradient descent on the objective in (1) —possibly with
a regularizer—which is the usual method to solve nonconvex problems in
machine learning. Remember, the matrix is <script type="math/tex">N \times  N</script>, i.e., rather
large!</p>

<p>But to the best of my knowledge, the following question had not been
raised or debated: <span><em>What property of human language explains the
fact that these very high-dimensional matrices of weighted word counts
are approximable by low-rank matrices?</em></span> (In a subsequent blog
post I will describe our new theoretical explanation.)</p>

<p>The third embedding method I wish to desribe uses so-called
<em>energy-based models</em>, exemplified by the famous papers on
<strong>Word2Vec</strong> around 2013 by the Google team of Mikolov et al..
They were inspired by pre-existing language models based upon neural nets. Let me
describe the simplest of the many variants. It assumes that the word
vectors are related to word probabilities as follows:</p>

<hr />

<p><em>Embedding 3</em> (<strong>Word2Vec(CBOW)</strong>):</p>

<p><script type="math/tex">\Pr[w|w_1, w_2, \ldots, w_5] \propto \exp(v_w \cdot (\frac{1}{5} \sum_i v_{w_i})),\qquad (2)</script>
where the left hand side gives the empirical probability that word <script type="math/tex">w</script> occurs in the text
conditional on the last five words being <script type="math/tex">w_1</script> through <script type="math/tex">w_5</script>.</p>

<hr />
<p>(Note that there is an implicit constraint capping the dimension of the
vectors at, say, 300.) Such models may look bizarre when you first see
them, but they work well in a host of applications. They are fitted
using nonconvex optimization, on very large corpora. (The optimization
involves a clever idea called <span><em>negative sampling</em></span>, but I
won’t go into details.)</p>

<p>In fact, the application of embeddings to word analogy tasks is also due
to the <strong>word2vec</strong> papers, and the reason why this word
created a big buzz.</p>

<p>The <strong>word2vec</strong> papers are a bit mysterious, and have motivated much
followup work. A paper by Levy and Goldberg explains that the <strong>word2vec</strong>
methods are actually modern versions of older vector space methods.
After all, if you take logs of both sides of expression (2), you see
that the <em>logarithm</em> of some cooccurence probability is
being expressed in terms of inner products of some word vectors, which
is very much in the spirit of the older work.</p>

<p>A paper by Pennington et al. at Stanford suggests a model called GLOVE
that uses an explicit weighted-SVD strategy for finding word embeddings.
In a future post I will talk more about the subsequent work; why word
embeddings solve analogy tasks; and some other cool properties of word
embeddings.</p>


</article>

<hr />


  <div class="share-page">
<div class="share-links" style="text-align:right;">
  Share this post:   
    
      <a class = "fa fa-facebook" href="https://facebook.com/sharer.php?u=http://offconvex.github.io/jekyll/pixyll/2014/06/11/word-embeddings/" rel="nofollow" target="_blank" title="Share on Facebook"></a>
    

    
      <a class="fa fa-twitter" href="https://twitter.com/intent/tweet?text=Semantic Word Embeddings&url=http://offconvex.github.io/jekyll/pixyll/2014/06/11/word-embeddings/" rel="nofollow" target="_blank" title="Share on Twitter"></a>
    

    
      <a class="fa fa-google-plus" href="https://plus.google.com/share?url=http://offconvex.github.io/jekyll/pixyll/2014/06/11/word-embeddings/" rel="nofollow" target="_blank" title="Share on Google+"></a>
    

    
      <a class="fa fa-linkedin" href="http://www.linkedin.com/shareArticle?url=http://offconvex.github.io/jekyll/pixyll/2014/06/11/word-embeddings/&title=Semantic Word Embeddings" rel="nofollow" target="_blank" title="Share on LinkedIn"></a>
    

    

    

    
      <a class="fa fa-reddit" href="http://reddit.com/submit?url=http://offconvex.github.io/jekyll/pixyll/2014/06/11/word-embeddings/&title=Semantic Word Embeddings" rel="nofollow" target="_blank" title="Share on Reddit"></a>
    

    

    
      <a class = "fa fa-hacker-news" onclick="parent.postMessage('submit','*')" href="https://news.ycombinator.com/submitlink?u=http://offconvex.github.io/jekyll/pixyll/2014/06/11/word-embeddings/&t=Semantic Word Embeddings" rel="nofollow" target="_blank" title="Share on Hacker News"></a>
    


  </div>
</div>






  <h3>Comments</h3>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_shortname  = 'offconvex';
    var disqus_identifier = '/jekyll/pixyll/2014/06/11/word-embeddings';
    var disqus_title      = 'Semantic Word Embeddings';

    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>






      </div>
    </div>
  </div>

  <footer class="center">
  <div class="measure">
    <small>
      Theme crafted with &lt;3 by <a href="http://johnotander.com">John Otander</a> (<a href="https://twitter.com/4lpine">@4lpine</a>).<br>
      &lt;/&gt; available on <a href="https://github.com/johnotander/pixyll">Github</a>.
    </small>
  </div>
</footer>

</body>
</html>
