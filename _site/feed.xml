<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Victoria Uni machine learning research group (insert witty title)</title>
    <description>Deeplearning at Vic. The happs, the peeps, ...</description>
    <link>http://festivalofdoubt.github.io/</link>
    <atom:link href="http://festivalofdoubt.github.io/feed.xml" rel="self" type="application/rss+xml" />
    
     
      <item>
        <title>Stability as a foundation of machine learning</title>
        <description>&lt;p&gt;Central to machine learning is our ability to relate how a learning algorithm fares on a sample to its performance on unseen instances. This is called &lt;em&gt;generalization&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In this post, I will describe a purely algorithmic approach to generalization. The property that makes this possible is &lt;em&gt;stability&lt;/em&gt;. An algorithm is &lt;em&gt;stable&lt;/em&gt;, intuitively speaking, if its output doesn’t change much if we perturb the input sample in a single point. We will see that this property by itself is necessary and sufficient for generalization.&lt;/p&gt;

&lt;h2 id=&quot;example-stability-of-the-perceptron-algorithm&quot;&gt;Example: Stability of the Perceptron algorithm&lt;/h2&gt;

&lt;p&gt;Before we jump into the formal details, let’s consider a simple example of a stable algorithm: The &lt;a href=&quot;https://en.wikipedia.org/wiki/Perceptron&quot;&gt;Perceptron&lt;/a&gt;, aka stochastic gradient descent for learning linear separators! The algorithm aims to separate two classes of points (here circles and triangles) with a linear separator. The algorithm starts with an arbitrary hyperplane. It then repeatedly selects a single example from its input set and updates its hyperplane using the gradient of a certain loss function on the chosen example. How bad might the algorithm screw up if we move around a single example? Let’s find out.&lt;/p&gt;

&lt;p&gt;&lt;!-- begin animation --&gt;&lt;/p&gt;
&lt;div style=&quot;text-align:center;&quot;&gt;
   &lt;img id=&quot;imganim&quot; src=&quot;/assets/sgd/00.png&quot; onclick=&quot;forward_image()&quot; /&gt;
   &lt;p style=&quot;text-align:center;&quot;&gt;&lt;em&gt;Step &lt;span style=&quot;font-family:monospace;&quot;&gt;&lt;span id=&quot;counter&quot;&gt;1&lt;/span&gt;/30&lt;/span&gt;. Click to advance.&lt;br /&gt; The animation shows two runs of the Perceptron algorithm for learning a linear separator on two data sets that differ in the one point marked green in one data set and purple in the other. The perturbation is indicated by an arrow. The shaded green region shows the difference in the resulting two hyperplanes after some number of steps. &lt;/em&gt;&lt;/p&gt;
  &lt;/div&gt;
&lt;script type=&quot;text/javascript&quot;&gt;//&lt;![CDATA[
  var images = [
  &quot;/assets/sgd/00.png&quot;,
  &quot;/assets/sgd/01.png&quot;,
  &quot;/assets/sgd/02.png&quot;,
  &quot;/assets/sgd/03.png&quot;,
  &quot;/assets/sgd/04.png&quot;,
  &quot;/assets/sgd/05.png&quot;,
  &quot;/assets/sgd/06.png&quot;,
  &quot;/assets/sgd/07.png&quot;,
  &quot;/assets/sgd/08.png&quot;,
  &quot;/assets/sgd/09.png&quot;,
  &quot;/assets/sgd/10.png&quot;,
  &quot;/assets/sgd/11.png&quot;,
  &quot;/assets/sgd/12.png&quot;,
  &quot;/assets/sgd/13.png&quot;,
  &quot;/assets/sgd/14.png&quot;,
  &quot;/assets/sgd/15.png&quot;,
  &quot;/assets/sgd/16.png&quot;,
  &quot;/assets/sgd/17.png&quot;,
  &quot;/assets/sgd/18.png&quot;,
  &quot;/assets/sgd/19.png&quot;,
  &quot;/assets/sgd/20.png&quot;, 
  &quot;/assets/sgd/21.png&quot;,
  &quot;/assets/sgd/22.png&quot;,
  &quot;/assets/sgd/23.png&quot;,
  &quot;/assets/sgd/24.png&quot;,
  &quot;/assets/sgd/25.png&quot;,
  &quot;/assets/sgd/26.png&quot;,
  &quot;/assets/sgd/27.png&quot;,
  &quot;/assets/sgd/28.png&quot;,
  &quot;/assets/sgd/29.png&quot; ]
  var i = 0
  function forward_image(){
   i = i + 1;
   document.getElementById(&#39;imganim&#39;).src = images[i%30];
   document.getElementById(&#39;counter&#39;).textContent = (i%30) + 1;
  }
  //]]&gt; 
  &lt;/script&gt;

&lt;p&gt;&lt;!-- end animation --&gt;&lt;/p&gt;

&lt;p&gt;As we can see by clicking impatiently through the example, the algorithm seems pretty stable. Even if we substantially move the first example it encounters, the hyperplane computed by the algorithm changes only slightly. Neat. (You can check out the code &lt;a href=&quot;https://gist.github.com/mrtzh/266c37d3a274376134a6&quot;&gt;here&lt;/a&gt;.)&lt;/p&gt;

&lt;h2 id=&quot;empirical-risk-jargon&quot;&gt;Empirical risk jargon&lt;/h2&gt;

&lt;p&gt;Let’s introduce some terminology to relate the behavior of an algorithm on a sample to its behavior on unseen instances. Imagine we have a sample $S=(z_1,\dots,z_n)$ drawn i.i.d. from some unknown distribution $D$. There’s a learning algorithm $A(S)$ that takes $S$ and produces some model (e.g., the hyperplane in the above picture). To quantify the quality of the model we crank out a &lt;em&gt;loss function&lt;/em&gt; $\ell$ with the idea that $\ell(A(S), z)$ describes the &lt;em&gt;loss&lt;/em&gt; of the model $A(S)$ on one instance $z$. The &lt;em&gt;empirical risk&lt;/em&gt; or &lt;em&gt;training error&lt;/em&gt; of the algorithm is defined as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_S = \frac1n \sum_{i=1}^n \ell(A(S), z_i)&lt;/script&gt;

&lt;p&gt;This captures the average loss of the algorithm on the sample on which it was trained. To quantify &lt;em&gt;out-of-sample&lt;/em&gt; performance, we define the &lt;em&gt;risk&lt;/em&gt; of the algorithm as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R = \mathop{\mathbb{E}}_{z\sim D}\left[ \ell(A(S), z) \right]&lt;/script&gt;

&lt;p&gt;The difference between risk and empirical risk $R - R_S$ is called &lt;em&gt;generalization error&lt;/em&gt;. You will sometimes encounter that term as a synonym for risk, but I find that confusing. We already have a perfectly short and good name for the risk $R$. Always keep in mind the following tautology&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R = R_S + (R-R_S).&lt;/script&gt;

&lt;p&gt;Operationally, it states that if we manage to minimize empirical risk all that matters is generalization error.&lt;/p&gt;

&lt;h2 id=&quot;a-fundamental-theorem-of-machine-learning&quot;&gt;A fundamental theorem of machine learning&lt;/h2&gt;

&lt;p&gt;I probably shouldn’t propose fundamental theorems for anything really. But if I had to, this would be the one I’d suggest for machine learning:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In expectation, generalization equals stability.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Somewhat more formally, we will encounter a natural measure of stability, denoted $\Delta$ such that the difference between risk and empirical risk in expectation equals $\Delta.$ Formally,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\mathbb{E}[R - R_S] = \Delta$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Deferring the exact definition of $\Delta$ to the proof, let’s think about this for a second.
What I find so remarkable about this theorem is that it turns a statistical problem into a purely algorithmic one: All we need for generalization is an algorithmic notion of robustness. Our algorithm’s output shouldn’t change much if perturb one of the data points. It’s almost like a sanity check. Had you coded up an algorithm and this wasn’t the case, you’d probably go look for a bug.&lt;/p&gt;

&lt;h3 id=&quot;proof&quot;&gt;Proof&lt;/h3&gt;

&lt;p&gt;Consider two data sets of size $n$ drawn independently of each other:
[
S = (z_1,\dots,z_n), \qquad S’=(z_1’,\dots,z_n’)
]
The idea of taking such a &lt;em&gt;ghost sample&lt;/em&gt; $S’$ is quite old and already arises in the context of &lt;em&gt;symmetrization&lt;/em&gt; in empirical process theory.
We’re going to couple these two samples in one point by defining
[
S^i = (z_1,\dots,z_{i-1},z_i’,z_{i+1},\dots,z_n),\qquad i = 1,\dots, n.
]
It’s certainly no coincidence that $S$ and $S^i$ differ in exactly one element. We’re going to use this in just a moment.&lt;/p&gt;

&lt;p&gt;By definition, the &lt;em&gt;expected empirical risk&lt;/em&gt; equals&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}[R_S] = \mathbb{E}\left[ \frac1n \sum_{i=1}^n \ell(A(S), z_i) \right].&lt;/script&gt;

&lt;p&gt;Contrasting this to how the algorithm fares on unseen examples, we can rewrite the &lt;em&gt;expected risk&lt;/em&gt; using our ghost sample as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}[R] = \mathbb{E}\left[ \frac1n \sum_{i=1}^n \ell(A(S), \color{red}{z_i&#39;}) \right]&lt;/script&gt;

&lt;p&gt;All expectations we encounter are over both $S$ and $S’$. By linearity of expectation, the difference between expected risk and expected empirical risk equals&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}[R - R_S] 
= \frac1n \sum_{i=1}^n 
\mathbb{E}\left[\ell(A(S), \color{red}{z_i&#39;})-\ell(A(S), z_i)\right].&lt;/script&gt;

&lt;p&gt;It is tempting now to relate the two terms inside the expectation to the stability of the algorithm. We’re going to do exactly that using mathematics’ most trusted proof strategy: &lt;em&gt;pattern matching&lt;/em&gt;. Indeed, since $z_i$ and $z_i’$ are exchangeable, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}[\ell(A(S), z_i)] 
= \mathbb{E}[\ell(A(S^i), z_i&#39;)]
= \mathbb{E}[\ell(A(S), z_i&#39;)] - \delta_i,&lt;/script&gt;

&lt;p&gt;where $\delta_i$ is defined to make the second equality true:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta_i = \mathbb{E}[\ell(A(\color{red}S), z_i&#39;)- \ell(A(\color{red}{S^i}), z_i&#39;)]&lt;/script&gt;

&lt;p&gt;Summing up $\Delta = (1/n)\sum_i \delta_i$, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}[ R - R_S ] = \Delta.&lt;/script&gt;

&lt;p&gt;The only thing left to do is to interpret the right hand side in terms of stability. Convince yourself that $\delta_i$ measures how differently the algorithm behaves on two data sets $S$ and $S’$ that differ in only one element.&lt;/p&gt;

&lt;h3 id=&quot;uniform-stability&quot;&gt;Uniform stability&lt;/h3&gt;

&lt;p&gt;It can be difficult to analyze the expectation in the definition of $\Delta$ precisely. Fortunately, it is often enough to resolve the expectation by upper bounding it with suprema:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;|\Delta| \le \sup_{S,S&#39;} \sup_{z} \left|\ell(A(S),z)-\ell(A(S&#39;),z)\right|.&lt;/script&gt;

&lt;p&gt;The supremum runs over all valid data sets differing in only one element and all valid sample points $z$. This stronger notion of stability called &lt;em&gt;uniform stability&lt;/em&gt; 
goes back to a seminal paper by Bousquett and Elisseeff.&lt;/p&gt;

&lt;p&gt;I should say that you can find the above proof in the essssential stability paper by Shalev-Shwartz, Shamir, Srebro and Sridharan &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume11/shalev-shwartz10a/shalev-shwartz10a.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;concentration-from-stability&quot;&gt;Concentration from stability&lt;/h3&gt;

&lt;p&gt;The theorem we saw shows that &lt;em&gt;expected&lt;/em&gt; empirical risk equals risk up to a correction that involves the stability of the algorithm. Can we also show that empirical risk is close to its expectation with high probability? Interestingly, we can by appealing to stability once again. I won’t spell out the details, but we can use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Doob_martingale#McDiarmid.27s_inequality&quot;&gt;method of bounded differences&lt;/a&gt; to obtain strong concentration bounds. To apply the method we need a &lt;em&gt;bounded difference&lt;/em&gt; condition which is just another word for &lt;em&gt;stability&lt;/em&gt;. So, we’re really killing two birds with one stone by using stability not only to show that the first moment of the empirical risk is correct but also that it concentrates. The only wrinkle is that, as far as I know, the weak stability notion expressed by $\Delta$ is not enough to get concentration, but uniform stability (for sufficiently small difference) will do.&lt;/p&gt;

&lt;h2 id=&quot;applications-of-stability&quot;&gt;Applications of stability&lt;/h2&gt;

&lt;p&gt;There is much more that stability can do for us. We’ve only scratched on the surface. Here are some of the many applications of stability.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume2/bousquet02a/bousquet02a.pdf&quot;&gt;Regularization implies stability&lt;/a&gt;. Specifically, the minimizer of the empirical risk subject to an $\ell_2$-penalty is uniformly stable.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://arxiv.org/abs/1509.01240&quot;&gt;Stochastic gradient descent is stable&lt;/a&gt; provided that we don’t make too many steps.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Differential privacy is nothing but a strong stability guarantee. Any result ever proved about differential privacy is fundamentally about stability.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Differential privacy in turn has applications to preventing overfitting in &lt;a href=&quot;http://blog.mrtz.org/2015/12/14/adaptive-data-analysis.html&quot;&gt;adaptive data analysis&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Stability also has many beautiful applications and connections in statistics. I strongly encourage you to read Bin Yu’s beautiful &lt;a href=&quot;https://www.stat.berkeley.edu/~binyu/ps/papers2013/Yu13.pdf&quot;&gt;overview paper&lt;/a&gt; on the topic.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Looking ahead, I’ve got at least two more posts planned on this.&lt;/p&gt;

&lt;p&gt;In my next post I will go into the stability of stochastic gradient descent in detail. We will see a simple argument to show that stochastic gradient descent is uniformly stable. I will then work towards applying these ideas to the area of deep learning. We will see that stability can help us explain why even huge models sometimes generalize well and how we can make them generalize even better.&lt;/p&gt;

&lt;p&gt;In a second post I will reflect on stability as a paradigm for reliable machine learning. The focus will be on how ideas from stability can help avoid overfitting and false discovery.&lt;/p&gt;
</description>
        <pubDate>Mon, 14 Mar 2016 21:00:00 +1300</pubDate>
        <link>http://festivalofdoubt.github.io/2016/03/14/stability/</link>
        <guid isPermaLink="true">http://festivalofdoubt.github.io/2016/03/14/stability/</guid>
      </item>
     
    
     
      <item>
        <title>Tensor Methods in Machine Learning</title>
        <description>&lt;p&gt;&lt;em&gt;Tensors&lt;/em&gt; are high dimensional generalizations of matrices. In recent years &lt;em&gt;tensor decompositions&lt;/em&gt; were used to design learning algorithms for estimating parameters of latent variable models like Hidden Markov Model, Mixture of Gaussians and Latent Dirichlet Allocation (many of these works were considered as examples of “spectral learning”, read on to find out why). In this post I will briefly describe why &lt;em&gt;tensors&lt;/em&gt; are useful in these settings.&lt;/p&gt;

&lt;p&gt;Using &lt;a href=&quot;https://en.wikipedia.org/wiki/Singular_value_decomposition&quot;&gt;Singular Value Decomposition (SVD)&lt;/a&gt;, we can write a matrix $M \in \mathbb{R}^{n\times m}$ as the sum of many rank one matrices:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;M = \sum_{i=1}^r \lambda_i \vec{u}_i \vec{v}_i^\top.&lt;/script&gt;

&lt;p&gt;When the &lt;em&gt;rank&lt;/em&gt; $r$ is small, this gives a concise representation for the matrix $M$ (using $(m+n)r$ parameters instead of $mn$). Such decompositions are widely applied in machine learning.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Tensor decomposition&lt;/em&gt; is a generalization of low rank matrix decomposition. Although &lt;a href=&quot;http://arxiv.org/abs/0911.1393&quot;&gt;most tensor problems are NP-hard&lt;/a&gt; in the worst case, several natural subcases of tensor decomposition can be solved in polynomial time. Later we will see that these subcases are still very powerful in learning latent variable models.&lt;/p&gt;

&lt;h2 id=&quot;matrix-decompositions&quot;&gt;Matrix Decompositions&lt;/h2&gt;

&lt;p&gt;Before talking about tensors, let us first see an example of how matrix factorization can be used to learn latent variable models. In 1904, psychologist Charles Spearman tried to understand whether human intelligence is a composite of different types of measureable intelligence.  Let’s describe a highly simplified version of his method, where the hypothesis is that there are exactly two kinds of intelligence: &lt;em&gt;quantitative&lt;/em&gt; and &lt;em&gt;verbal&lt;/em&gt;. Spearman’s method consisted of making his subjects take several different kinds of tests. 
Let’s name these tests &lt;em&gt;Classics, Math, Music&lt;/em&gt;, etc. The subjects scores can be represented by a &lt;em&gt;matrix&lt;/em&gt; $M$, which has one row per student, and one column per test.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/matrix.png&quot; alt=&quot;matrix M&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The simplified version of Spearman’s hypothesis is that each student has different amounts of quantitative and verbal intelligence, say  $x_{quant}$ and $x_{verb}$ respectively. Each test measures a different mix of intelligences, so say it gives a &lt;em&gt;weighting&lt;/em&gt; $y_{quant}$ to quantitative and $y_{verb}$ to verbal. Intuitively, a student with higher strength on verbal intelligence should perform better on a test that has a high weight on verbal intelligence. Let’s describe this relationship as a simple &lt;em&gt;bilinear&lt;/em&gt; function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;score = x_{quant} \times y_{quant} + x_{verb}\times y_{verb}.&lt;/script&gt;

&lt;p&gt;Denoting by $\vec x_{verb}, \vec x_{quant}$ the vectors describing the strengths of the students, and letting $\vec y_{verb}, \vec y_{quant}$ be the vectors that describe the weighting of intelligences in the different tests, we can express matrix $M$ as the sum of two &lt;strong&gt;rank 1&lt;/strong&gt; matrices (in other words, $M$ has rank at most $2$):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;M = \vec x_{quant} \vec y_{quant}^\top + \vec x_{verb} \vec y_{verb}^\top.&lt;/script&gt;

&lt;p&gt;Thus verifying that $M$ has rank $2$ (or that it is very close to a rank $2$ matrix) should let us conclude that there are indeed two kinds of intelligence.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/decomposition1.png&quot; alt=&quot;Matrix Decomposition&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that this decomposition is not the &lt;em&gt;Singular Value Decomposition&lt;/em&gt; (SVD). SVD requires strong orthogonality constraints (which translates to “different intelligences are completely uncorrelated”) that are not plausible in this setting.&lt;/p&gt;

&lt;h2 id=&quot;the-ambiguity&quot;&gt;The Ambiguity&lt;/h2&gt;

&lt;p&gt;But ideally one would like to take the above idea further: we would like to assign a definitive quantitative/verbal intelligence score to each student. This seems simple at first sight: just read off the score from the decomposition. For instance, it shows Alice is strongest in quantitative intelligence.&lt;/p&gt;

&lt;p&gt;However, this is incorrect, because the decomposition is &lt;strong&gt;not&lt;/strong&gt; unique! The following is another valid decomposition&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/decomposition2.png&quot; alt=&quot;Matrix Decomposition&quot; /&gt;&lt;/p&gt;

&lt;p&gt;According to this decomposition, Bob is strongest in quantitative intelligence, not Alice. Both decompositions explain the data perfectly and we &lt;strong&gt;cannot&lt;/strong&gt; decide &lt;em&gt;a priori&lt;/em&gt; which is correct.&lt;/p&gt;

&lt;p&gt;Sometimes we can hope to find the unique solution by imposing additional constraints on the decomposition, such as all matrix entries have to be nonnegative. However even after imposing many natural constraints, in general the issue of multiple decompositions will remain.&lt;/p&gt;

&lt;h2 id=&quot;adding-the-3rd-dimension&quot;&gt;Adding the 3rd Dimension&lt;/h2&gt;

&lt;p&gt;Since our current data has multiple explanatory decompositions, we need  more data to learn exactly which explanation is the truth. Assume the strength of the intelligence changes with time: we get better at quantitative tasks at night. Now we can let the (poor) students take the tests twice: once during the day and once at night. The results we get can be represented by two matrices $M_{day}$ and $M_{night}$. But we can also think of this as a three dimensional array of numbers -– a tensor $T$ in $\mathbb{R}^{\sharp students\times \sharp tests\times 2}$.
Here the third axis stands for “day” or “night”. We say the two matrices $M_{day}$ and $M_{night}$ are &lt;em&gt;slices&lt;/em&gt; of the tensor $T$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bimatrix.png&quot; alt=&quot;Matrix Decomposition&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let $z_{quant}$ and $z_{verb}$ be the relative strength of the two kinds of intelligence at a particular time (day or night), then the new score can be computed by a &lt;em&gt;trilinear&lt;/em&gt; function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;score = x_{quant} \times y_{quant} \times z_{quant} + x_{verb}\times y_{verb} \times z_{verb}.&lt;/script&gt;

&lt;p&gt;Keep in mind that this is the formula for &lt;strong&gt;one entry&lt;/strong&gt; in the tensor: the score of one student, in one test and at a specific time. Who the student is specifies $x_{quant}$ and $x_{verb}$; what the test is specifies weights $y_{quant}$ and $y_{verb}$; when the test takes place specifies $z_{quant}$ and $z_{verb}$.&lt;/p&gt;

&lt;p&gt;Similar to matrices, we can view this as a &lt;strong&gt;rank 2&lt;/strong&gt; decomposition of the tensor $T$. In particular, if we use $\vec x_{quant}, \vec x_{verb}$ to denote the strengths of students, $\vec y_{quant},\vec y_{verb}$ to denote the weights of the tests and $\vec z_{quant}, \vec z_{verb}$ to denote the variations of strengths in time, then we can write the decomposition as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T = \vec x_{quant}\otimes \vec y_{quant}\otimes \vec z_{quant} + \vec x_{verb}\otimes \vec y_{verb}\otimes \vec z_{verb}.&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tensordecomposition.png&quot; alt=&quot;Matrix Decomposition&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now we can check that the second matrix decomposition we had is no longer valid: there are no values of $z_{quant}$ and $z_{verb}$ at night that could generate the matrix $M_{night}$. This is not a coincidence. &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/0024379577900696&quot;&gt;Kruskal 1977&lt;/a&gt; gave sufficient conditions for such decompositions to be unique. When applied to our case it is very simple:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Corollary&lt;/strong&gt;
The decomposition of tensor $T$ is unique (up to scaling and permutation) if none of the vector pairs $(\vec x_{quant}, \vec x_{verb})$, $(\vec y_{quant},\vec y_{verb})$, $(\vec z_{quant},\vec z_{verb})$ are co-linear.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Note that of course the decomposition is not truly unique for two reasons. First, the two tensor factors are symmetric, and we need to decide which factor correspond to quantitative intelligence. Second, we can scale the three components $\vec x_{quant}$ ,$\vec y_{quant}$, $\vec z_{quant}$ simultaneously, as long as the product of the three scales is 1. Intuitively this is like using different units to measure the three components. Kruskal’s result showed that these are the only degrees of freedom in the decomposition, and there cannot be a truly distinct decomposition as in the matrix case.&lt;/p&gt;

&lt;h2 id=&quot;finding-the-tensor&quot;&gt;Finding the Tensor&lt;/h2&gt;

&lt;p&gt;In the above example we get a low rank tensor $T$ by gathering more data. In many traditional applications the extra data may be unavailable or hard to get. Luckily, many exciting recent developments show that we can &lt;em&gt;uncover&lt;/em&gt; these special tensor structures even if the original data is not in a tensor form!&lt;/p&gt;

&lt;p&gt;The main idea is to use &lt;em&gt;method of moments&lt;/em&gt; (see a nice &lt;a href=&quot;http://blog.mrtz.org/2014/04/22/pearsons-polynomial.html&quot;&gt;post by Moritz&lt;/a&gt;): estimate lower order correlations of the variables, and hope these lower order correlations have a simple tensor form.&lt;/p&gt;

&lt;p&gt;Consider &lt;em&gt;Hidden Markov Model&lt;/em&gt; as an example. Hidden Markov Models are widely used in analyzing sequential data like speech or text. Here for concreteness we consider a (simplified) model of natural language texts(which is a basic version of the &lt;a href=&quot;http://www.offconvex.org/2015/12/12/word-embeddings-1/&quot;&gt;&lt;em&gt;word embeddings&lt;/em&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;In Hidden Markov Model, we observe a sequence of words (a sentence) that is generated by a walk of a &lt;em&gt;hidden&lt;/em&gt; Markov Chain: each word has a hidden topic $h$ (a discrete random variable that specifies whether the current word is talking about “sports” or “politics”); the topic for the next word only depends on the topic of the current word. Each topic specifies a distribution over words. Instead of the topic itself, we observe a random word $x$ drawn from this topic distribution (for example, if the topic is “sports”, we will more likely see words like “score”). The dependencies are usually illustrated by the following diagram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/hmm.png&quot; alt=&quot;Hidden Markov Model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;More concretely, to generate a sentence in Hidden Markov Model, we start with some initial &lt;em&gt;topic&lt;/em&gt; $h_1$. This topic will evolve as a Markov Chain to generate the topics for future words $h_2, h_3,…,h_t$. We observe &lt;em&gt;words&lt;/em&gt; $x_1,…,x_t$ from these topics. In particular, word $x_1$ is drawn according to topic $h_1$, word $x_2$ is drawn according to topic $h_2$ and so on.&lt;/p&gt;

&lt;p&gt;Given many sentences that are &lt;em&gt;generated exactly&lt;/em&gt; according to this model, how can we construct a tensor? A natural idea is to compute &lt;em&gt;correlations&lt;/em&gt;: for every triple of words $(i,j,k)$, we count the number of times that these are the first three words of a sentence. Enumerating over $i,j,k$ gives us a three dimensional array (a &lt;em&gt;tensor&lt;/em&gt;) $T$. We can further normalize it by the total number of sentences. After normalization the $(i,j,k)$-th entry of the tensor will be an estimation of the &lt;em&gt;probability&lt;/em&gt; that the first three words are $(i,j,k)$. For simplicity assume we have enough samples and the estimation is accurate:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T_{i,j,k} = \mbox{Pr}[x_1 = i, x_2=j, x_3=k].&lt;/script&gt;

&lt;p&gt;Why does this tensor have the nice low rank property? The key observation is that if we “fix” (condition on) the topic of the second word $h_2$, it cuts the graph into three parts: one part containing $h_1,x_1$, one part containing $x_2$ and one part containing $h_3,x_3$. These three parts are &lt;strong&gt;independent&lt;/strong&gt; conditioned on $h_2$. In particular, the first three words $x_1,x_2,x_3$ are &lt;strong&gt;independent&lt;/strong&gt; conditioned on the topic of the &lt;strong&gt;second&lt;/strong&gt; word $h_2$. Using this observation we can compute each entry of the tensor as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T_{i,j,k} = \sum_{l=1}^n \mbox{Pr}[h_2 = l] \mbox{Pr}[x_1 = i| h_2 = l]\times \mbox{Pr}[x_2 = j| h_2 = l]\times \mbox{Pr}[x_3 = k| h_2 = l].&lt;/script&gt;

&lt;p&gt;Now if we let $\vec x_l$ be a vector whose $i$-th entry is the probability of the first word is $i$, given the topic of the &lt;em&gt;second&lt;/em&gt; word is $l$; let $\vec y_l$ and $\vec z_l$ be similar for the second and third word. We can then write the entire tensor as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T = \sum_{l=1}^n \mbox{Pr}[h_2 = l] \vec x_l \otimes \vec y_l \otimes \vec z_l.&lt;/script&gt;

&lt;p&gt;This is exactly the &lt;strong&gt;low rank&lt;/strong&gt; form we are looking for! Tensor decomposition allows us to &lt;em&gt;uniquely&lt;/em&gt; identify these components, and further infer the other probabilities we are interested in. For more details see the paper by &lt;a href=&quot;http://arxiv.org/abs/1210.7559&quot;&gt;Anandkumar et al. 2012&lt;/a&gt; (this paper uses the tensor notations, but the original idea appeared in the paper by &lt;a href=&quot;https://projecteuclid.org/euclid.aoap/1151592244&quot;&gt;Mossel and Roch 2006&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&quot;implementing-tensor-decomposition&quot;&gt;Implementing Tensor Decomposition&lt;/h2&gt;

&lt;p&gt;Using method of moments, we can discover nice tensor structures from many problems. The uniqueness of tensor decomposition makes these tensors very useful in learning the parameters of the models. But how do we compute the tensor decompositions?&lt;/p&gt;

&lt;p&gt;In the worst case we have bad news: &lt;a href=&quot;http://arxiv.org/abs/0911.1393&quot;&gt;most tensor problems are NP-hard&lt;/a&gt;. However, in most natural cases, as long as the tensor does &lt;em&gt;not&lt;/em&gt; have &lt;em&gt;too many&lt;/em&gt; components, and the components are &lt;em&gt;not adversarially&lt;/em&gt; chosen, tensor decomposition &lt;strong&gt;can&lt;/strong&gt; be computed in polynomial time! Here we describe the algorithm by Dr. Robert Jenrich (it first appeared in a 1970 working paper by &lt;a href=&quot;http://hbanaszak.mjr.uw.edu.pl/TempTxt/Harshman_1970_Foundations%20of%20PARAFAC%20Procedure%20MOdels%20and%20Conditions%20for%20an%20Expalanatory%20Multimodal%20Factor%20Analysis.pdf&quot;&gt;Harshman&lt;/a&gt;, the version we present here is a more general version by &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=173234&quot;&gt;Leurgans, Ross and Abel 1993&lt;/a&gt;).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Jenrich’s Algorithm &lt;br /&gt;
Input: tensor $T = \sum_{i=1}^r \lambda_i \vec x_i \otimes \vec y_i \otimes \vec z_i$.&lt;/p&gt;

  &lt;ol&gt;
    &lt;li&gt;Pick two random vectors $\vec u, \vec v$.&lt;/li&gt;
    &lt;li&gt;Compute $T_\vec u = \sum_{i=1}^n u_i T[:,:,i] = \sum_{i=1}^r \lambda_i (\vec u^\top \vec z_i) \vec x_i \vec y_i^\top$.&lt;/li&gt;
    &lt;li&gt;Compute $T_\vec v = \sum_{i=1}^n v_i T[:,:,i] = \sum_{i=1}^r \lambda_i (\vec v^\top \vec z_i) \vec x_i \vec y_i^\top$.&lt;/li&gt;
    &lt;li&gt;$\vec x_i$’s are eigenvectors of $T_\vec u (T_\vec v)^{+}$, $\vec y_i$’s are eigenvectors of $T_\vec v (T_\vec u)^{+}$.&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the algorithm, “$^+$” denotes &lt;em&gt;pseudo-inverse&lt;/em&gt; of a matrix (think of it as inverse if this is not familiar).&lt;/p&gt;

&lt;p&gt;The algorithm looks at weighted &lt;em&gt;slices&lt;/em&gt; of the tensor: a weighted slice is a matrix that is the projection of the tensor along the $z$ direction (similarly if we take a &lt;em&gt;slice&lt;/em&gt; of a matrix $M$, it will be a vector that is equal to $M\vec u$). Because of the low rank structure, all the slices must share matrix decompositions with the &lt;strong&gt;same&lt;/strong&gt; components.&lt;/p&gt;

&lt;p&gt;The main observation of the algorithm is that although a &lt;em&gt;single&lt;/em&gt; matrix can have infinitely many low rank decompositions, &lt;em&gt;two&lt;/em&gt; matrices can only have a &lt;strong&gt;unique&lt;/strong&gt; decomposition if we require them to have the same components. In fact, it is highly unlikely for two &lt;em&gt;arbitrary&lt;/em&gt; matrices to share decompositions with the same components. In the tensor case, because of the low rank structure we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T_\vec u = XD_\vec u Y^\top; \quad T_\vec v = XD_\vec v Y^\top,&lt;/script&gt;

&lt;p&gt;where $D_\vec u,D_\vec v$ are diagonal matrices. This is called a &lt;em&gt;simultaneous diagonalization&lt;/em&gt; for $T_\vec u$ and $T_\vec v$. With this structure it is easy to show that $\vec x_i$’s are eigenvectors of $T_\vec u (T_\vec v)^{+} = X D_\vec u D_\vec v^{-1} X^+$. So we can actually compute &lt;em&gt;tensor decompositions&lt;/em&gt; using &lt;em&gt;spectral decompositions&lt;/em&gt; for matrices.&lt;/p&gt;

&lt;p&gt;Many of the earlier works (including &lt;a href=&quot;https://projecteuclid.org/euclid.aoap/1151592244&quot;&gt;Mossel and Roch 2006&lt;/a&gt;) that apply tensor decompositions to learning problems have actually independently &lt;em&gt;rediscovered&lt;/em&gt; this algorithm, and the word “tensor” never appeared in the papers. In fact,  tensor decomposition techniques are traditionally called “spectral learning” since they are seen as derived from SVD.  But now we have other methods to do tensor decompositions that have better theoretical guarantees and practical performances. See the survey by &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1655230&quot;&gt;Kolda and Bader 2009&lt;/a&gt; for more discussions.&lt;/p&gt;

&lt;h3 id=&quot;related-links&quot;&gt;Related Links&lt;/h3&gt;

&lt;p&gt;For more examples of using &lt;em&gt;tensor decompositions&lt;/em&gt; to learn latent variable models, see the paper by &lt;a href=&quot;http://arxiv.org/abs/1210.7559&quot;&gt;Anandkumar et al. 2012&lt;/a&gt;. This paper shows that several prior algorithms for learning models such as Hidden Markov Model, Latent Dirichlet Allocation, Mixture of Gaussians and Independent Component Analysis can be interpreted as doing tensor decompositions. The paper also gives a proof that &lt;em&gt;tensor power method&lt;/em&gt; is efficient and robust to noise.&lt;/p&gt;

&lt;p&gt;Recent research focuses on two problems: how to formulate other learning problems as tensor decompositions, and how to compute tensor decompositions under weaker assumptions. Using tensor decompositions, we can learn more models that include &lt;a href=&quot;http://arxiv.org/abs/1302.2684&quot;&gt;community models&lt;/a&gt;, &lt;a href=&quot;http://www.cs.columbia.edu/~mcollins/papers/uai2014-long.pdf&quot;&gt;probabilistic Context-Free-Grammars&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/abs/1503.00424&quot;&gt;mixture of general Gaussians&lt;/a&gt; and &lt;a href=&quot;http://newport.eecs.uci.edu/anandkumar/pubs/NN_GeneralizationBound.pdf&quot;&gt;two-layer neural networks&lt;/a&gt;. We can also efficiently compute tensor decompositions when the &lt;em&gt;rank&lt;/em&gt; of the tensor is much larger than the dimension (see for example the papers by &lt;a href=&quot;http://arxiv.org/abs/1311.3651&quot;&gt;Bhaskara et al. 2014&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/abs/1306.5825&quot;&gt;Goyal et al. 2014&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/abs/1504.05287&quot;&gt;Ge and Ma 2015&lt;/a&gt;). There are many other interesting works and open problems, and the list here is by no means complete.&lt;/p&gt;
</description>
        <pubDate>Fri, 18 Dec 2015 04:00:00 +1300</pubDate>
        <link>http://festivalofdoubt.github.io/2015/12/18/tensor-decompositions/</link>
        <guid isPermaLink="true">http://festivalofdoubt.github.io/2015/12/18/tensor-decompositions/</guid>
      </item>
     
    
     
    
  </channel>
</rss>
