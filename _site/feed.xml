<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Off the convex path</title>
    <description>Algorithms off the convex path.</description>
    <link>http://offconvex.github.io/</link>
    <atom:link href="http://offconvex.github.io/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Semantic Word Embeddings</title>
        <description>&lt;p&gt;This post can be seen as an introduction to why nonconvex problems arise
naturally in practice, and also the relative ease by which they are often
solved.&lt;/p&gt;

&lt;p&gt;The topic is &lt;em&gt;word embeddings&lt;/em&gt;, a geometric way to capture
the “meaning” of a word via a low-dimensional vector. They are useful in
many tasks in Information Retrieval (IR) and Natural Language Processing
(NLP), for example answering search queries or translating from one
language to another.&lt;/p&gt;

&lt;p&gt;You may wonder: how can a 300-dimensional vector capture the many
nuances of word meaning? And what the heck does this mean?&lt;/p&gt;

&lt;p&gt;A simple property of embeddings obtained by all the methods I’ll
describe is &lt;em&gt;cosine similarity&lt;/em&gt;: the  &lt;em&gt;similarity&lt;/em&gt; between two words 
(as rated by humans on a &lt;script type=&quot;math/tex&quot;&gt;[-1,1]&lt;/script&gt; scale) correlates with the &lt;em&gt;cosine&lt;/em&gt;
of the angle between their vectors. To 
give an example, the cosine for &lt;em&gt;milk&lt;/em&gt; and
&lt;em&gt;cow&lt;/em&gt; may be &lt;script type=&quot;math/tex&quot;&gt;0.7&lt;/script&gt;, whereas for &lt;em&gt;milk&lt;/em&gt; and
&lt;em&gt;stone&lt;/em&gt; it may be &lt;script type=&quot;math/tex&quot;&gt;-0.1&lt;/script&gt;, which is roughly the similarity
human subjects assign to them.&lt;/p&gt;

&lt;p&gt;A more interesting property in recent embeddings is that they can solve
&lt;em&gt;analogy&lt;/em&gt; relationships via linear algebra on embeddings.
For example, the word analogy question
&lt;em&gt;man : woman ::king : ??&lt;/em&gt; can be solved by looking for the
word &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;v_{king} - v_w&lt;/script&gt; is most similar to
&lt;script type=&quot;math/tex&quot;&gt;v_{man} - v_{woman}&lt;/script&gt;; in other words, minimizes&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;||v_w - v_{king} + v_{man} - v_{woman}||^2.&lt;/script&gt;

&lt;p&gt;This simple idea can solve &lt;script type=&quot;math/tex&quot;&gt;75\%&lt;/script&gt; of analogy questions on some standard testbed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/linearrelations.jpg&quot; alt=&quot;linear&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Good embeddings have other properties that will be covered in a future
post. Now let’s discuss simple methods to construct them.&lt;/p&gt;

&lt;p&gt;The methods all ultimately rely on &lt;em&gt;Firth’s hypothesis&lt;/em&gt;
from 1954, which says that Namely, the meaning of a word is determined
by the distribution of other words around it. To give an example, if I
ask you to think of a word that tends to co-occur with &lt;em&gt;cow,
drink, babies, calcium&lt;/em&gt;, you would immediately answer:
&lt;em&gt;milk&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Note that I am not suggesting that Firth’s hypothesis fully accounts
for all aspects of semantics. (If it it did, a computer would be able to
learn language completely in an unsupervised way by processing a large
enough text corpus. Many experts doubt this is possible: they feel
language –e.g. understanding new metaphors or jokes— requires some
knowledge of the physical world, as well as some $2$-way interaction
between teacher and student.) But it does imply a very simple 
word embedding, albeit a very high-dimensional one.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Embedding 1&lt;/em&gt;: Suppose the dictionary has &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; distinct words (in practice, &lt;script type=&quot;math/tex&quot;&gt;N =100,000&lt;/script&gt;). Take a very large text corpus (e.g., Wikipedia) and let &lt;script type=&quot;math/tex&quot;&gt;Count_5(w_1, w_2)&lt;/script&gt; be the number of times &lt;script type=&quot;math/tex&quot;&gt;w_1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;w_2&lt;/script&gt; occur within a distance &lt;script type=&quot;math/tex&quot;&gt;5&lt;/script&gt; of each other in the corpus. Then the word embedding for a word &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; is a vector of dimension &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;, with one coordinate for each dictionary word. The coordinate corresponding to word &lt;script type=&quot;math/tex&quot;&gt;w_2&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;Count_5(w, w_2)&lt;/script&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The obvious problem with Embedding 1 is that it uses
extremely high-dimensional vectors. How can we compress them?&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Embedding 2&lt;/em&gt;: Do dimension reduction by taking the rank-&lt;script type=&quot;math/tex&quot;&gt;300&lt;/script&gt;
singular value decomposition (SVD) of the above vectors.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Recall that for an &lt;script type=&quot;math/tex&quot;&gt;N \times N&lt;/script&gt; matrix &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt; this means finding vectors
&lt;script type=&quot;math/tex&quot;&gt;v_1, v_2, \ldots, v_N \in \mathbb{R}^{300}&lt;/script&gt; that minimize&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{ij} (M_{ij} - v_i \cdot v_j)^2 \qquad (1).&lt;/script&gt;

&lt;p&gt;Using SVD to do dimension reduction seems an obvious idea today but it
actually is not since it is unclear &lt;em&gt;a priori&lt;/em&gt; why the
above &lt;script type=&quot;math/tex&quot;&gt;N \times N&lt;/script&gt; matrix of cooccurance counts should be close to a
rank-300 matrix. This was empirically discovered in the paper on
&lt;em&gt;Latent Semantic Analysis&lt;/em&gt; or LSA. (It shows in fact that
low-dimensional embeddings are &lt;em&gt;better&lt;/em&gt; than
high-dimensional ones.)&lt;/p&gt;

&lt;p&gt;A research area called &lt;em&gt;Vector Space Models&lt;/em&gt; (see survey by
Turney and Pantel) studies various modifications of the above idea, most
of which involve reweighting the above raw counts: some buzzwords are
TF-IDF, PMI, Logarithm, Square-root, etc. In general reweighting the
&lt;script type=&quot;math/tex&quot;&gt;(i, j)&lt;/script&gt; term in expression (1) leads to a &lt;em&gt;weighted&lt;/em&gt;
version of SVD, which is NP-hard. (I always emphasize to my students
what a miraculous algorithm SVD is, since modifying the problem
statement in small ways makes it NP-hard.) But in practice, it can be
solved by simple gradient descent on the objective in (1) —possibly with
a regularizer—which is the usual method to solve nonconvex problems in
machine learning. Remember, the matrix is &lt;script type=&quot;math/tex&quot;&gt;N \times  N&lt;/script&gt;, i.e., rather
large!&lt;/p&gt;

&lt;p&gt;But to the best of my knowledge, the following question had not been
raised or debated: &lt;span&gt;&lt;em&gt;What property of human language explains the
fact that these very high-dimensional matrices of weighted word counts
are approximable by low-rank matrices?&lt;/em&gt;&lt;/span&gt; (In a subsequent blog
post I will describe our new theoretical explanation.)&lt;/p&gt;

&lt;p&gt;The third embedding method I wish to desribe uses so-called
&lt;em&gt;energy-based models&lt;/em&gt;, exemplified by the famous papers on
&lt;strong&gt;Word2Vec&lt;/strong&gt; around 2013 by the Google team of Mikolov et al..
They were inspired by pre-existing language models based upon neural nets. Let me
describe the simplest of the many variants. It assumes that the word
vectors are related to word probabilities as follows:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Embedding 3&lt;/em&gt; (&lt;strong&gt;Word2Vec(CBOW)&lt;/strong&gt;):&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pr[w|w_1, w_2, \ldots, w_5] \propto \exp(v_w \cdot (\frac{1}{5} \sum_i v_{w_i})),\qquad (2)&lt;/script&gt;

  &lt;p&gt;where the left hand side gives the empirical probability that word &lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; occurs in the text
conditional on the last five words being &lt;script type=&quot;math/tex&quot;&gt;w_1&lt;/script&gt; through &lt;script type=&quot;math/tex&quot;&gt;w_5&lt;/script&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;(Note that there is an implicit constraint capping the dimension of the
vectors at, say, 300.) Such models may look bizarre when you first see
them, but they work well in a host of applications. They are fitted
using nonconvex optimization, on very large corpora. (The optimization
involves a clever idea called &lt;span&gt;&lt;em&gt;negative sampling&lt;/em&gt;&lt;/span&gt;, but I
won’t go into details.)&lt;/p&gt;

&lt;p&gt;In fact, the application of embeddings to word analogy tasks is also due
to the &lt;strong&gt;word2vec&lt;/strong&gt; papers, and the reason why this word
created a big buzz.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;word2vec&lt;/strong&gt; papers are a bit mysterious, and have motivated much
followup work. A paper by Levy and Goldberg explains that the &lt;strong&gt;word2vec&lt;/strong&gt;
methods are actually modern versions of older vector space methods.
After all, if you take logs of both sides of expression (2), you see
that the &lt;em&gt;logarithm&lt;/em&gt; of some cooccurence probability is
being expressed in terms of inner products of some word vectors, which
is very much in the spirit of the older work.&lt;/p&gt;

&lt;p&gt;A paper by Pennington et al. at Stanford suggests a model called GLOVE
that uses an explicit weighted-SVD strategy for finding word embeddings.
In a future post I will talk more about the subsequent work; why word
embeddings solve analogy tasks; and some other cool properties of word
embeddings.&lt;/p&gt;

</description>
        <pubDate>Tue, 01 Dec 2015 01:00:00 -0800</pubDate>
        <link>http://offconvex.github.io/language/words/embeddings/meaning/2015/12/01/word-embeddings-1/</link>
        <guid isPermaLink="true">http://offconvex.github.io/language/words/embeddings/meaning/2015/12/01/word-embeddings-1/</guid>
      </item>
    
      <item>
        <title>How to write a post for this web site</title>
        <description>&lt;h1 id=&quot;basics&quot;&gt;Basics&lt;/h1&gt;

&lt;p&gt;Posts are written in markdown. The variant of markdown we use is called ‘kramdown’. Here is a &lt;a href=&quot;http://kramdown.gettalong.org/quickref.html&quot;&gt;quick reference&lt;/a&gt;. What you just saw was the syntax for a hyperlink in markdown.&lt;/p&gt;

&lt;p&gt;You can include an image like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/logo.png&quot; alt=&quot;logo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Below is a subsection.&lt;/p&gt;

&lt;h2 id=&quot;lists&quot;&gt;Lists&lt;/h2&gt;

&lt;p&gt;This is a list:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;One&lt;/li&gt;
  &lt;li&gt;Two&lt;/li&gt;
  &lt;li&gt;Three&lt;/li&gt;
  &lt;li&gt;and so on&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is an enumerated list:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;one&lt;/li&gt;
  &lt;li&gt;two&lt;/li&gt;
  &lt;li&gt;three&lt;/li&gt;
  &lt;li&gt;four&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;using-math&quot;&gt;Using math&lt;/h1&gt;

&lt;h2 id=&quot;inline-formulas&quot;&gt;Inline formulas&lt;/h2&gt;

&lt;p&gt;Indicate inline math like this.
Let &lt;script type=&quot;math/tex&quot;&gt;f\colon\mathbb{R}^n\to\mathbb{R}&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;display-formulas&quot;&gt;Display formulas&lt;/h2&gt;

&lt;p&gt;To use a display formula instead of inline math, simple offset it to a new line.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(0) = 0&lt;/script&gt;

&lt;p&gt;That’s all.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Blah&lt;/p&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code&gt;
test

&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;Why is this font so large? &lt;script type=&quot;math/tex&quot;&gt;1+1=2&lt;/script&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;using-custom-html&quot;&gt;Using custom html&lt;/h1&gt;

&lt;div style=&quot;border:1px solid #333; margin:10px 10%; padding: 10px 5%;&quot;&gt;

Embedding 1: $$1 + 1 = 2$$.

&lt;/div&gt;
</description>
        <pubDate>Sat, 11 Jul 2015 00:00:00 -0700</pubDate>
        <link>http://offconvex.github.io/howto/markdown/2015/07/11/word-embeddings-2/</link>
        <guid isPermaLink="true">http://offconvex.github.io/howto/markdown/2015/07/11/word-embeddings-2/</guid>
      </item>
    
      <item>
        <title>Mission statement</title>
        <description>&lt;p&gt;There is a significant amount of subtle, yet precisely calibrated, styling to ensure
that your content is emphasized while still looking aesthetically pleasing.&lt;/p&gt;

&lt;p&gt;All links are easy to &lt;a href=&quot;#&quot;&gt;locate and discern&lt;/a&gt;, yet don’t detract from the harmony
of a paragraph. The &lt;em&gt;same&lt;/em&gt; goes for italics and &lt;strong&gt;bold&lt;/strong&gt; elements. Even the the strikeout
works if &lt;del&gt;for some reason you need to update your post&lt;/del&gt;. For consistency’s sake,
&lt;ins&gt;The same goes for insertions&lt;/ins&gt;, of course.&lt;/p&gt;

&lt;h3 id=&quot;code-with-syntax-highlighting&quot;&gt;Code, with syntax highlighting&lt;/h3&gt;

&lt;p&gt;Here’s an example of some ruby code with line anchors.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;a name=&quot;True-1&quot;&gt;&lt;/a&gt;&lt;span class=&quot;c1&quot;&gt;# The most awesome of classes&lt;/span&gt;
&lt;a name=&quot;True-2&quot;&gt;&lt;/a&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Awesome&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;ActiveRecord&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;Base&lt;/span&gt;
&lt;a name=&quot;True-3&quot;&gt;&lt;/a&gt;  &lt;span class=&quot;kp&quot;&gt;include&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;EvenMoreAwesome&lt;/span&gt;
&lt;a name=&quot;True-4&quot;&gt;&lt;/a&gt;
&lt;a name=&quot;True-5&quot;&gt;&lt;/a&gt;  &lt;span class=&quot;n&quot;&gt;validates_presence_of&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:something&lt;/span&gt;
&lt;a name=&quot;True-6&quot;&gt;&lt;/a&gt;  &lt;span class=&quot;n&quot;&gt;validates&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;:email&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;ss&quot;&gt;email_format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;true&lt;/span&gt;
&lt;a name=&quot;True-7&quot;&gt;&lt;/a&gt;
&lt;a name=&quot;True-8&quot;&gt;&lt;/a&gt;  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;initialize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;email&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kp&quot;&gt;nil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;a name=&quot;True-9&quot;&gt;&lt;/a&gt;    &lt;span class=&quot;nb&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;email&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;email&lt;/span&gt;
&lt;a name=&quot;True-10&quot;&gt;&lt;/a&gt;    &lt;span class=&quot;nb&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;
&lt;a name=&quot;True-11&quot;&gt;&lt;/a&gt;    &lt;span class=&quot;nb&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;favorite_number&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;
&lt;a name=&quot;True-12&quot;&gt;&lt;/a&gt;    &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;created awesomeness&amp;#39;&lt;/span&gt;
&lt;a name=&quot;True-13&quot;&gt;&lt;/a&gt;  &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;a name=&quot;True-14&quot;&gt;&lt;/a&gt;
&lt;a name=&quot;True-15&quot;&gt;&lt;/a&gt;  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;email_format&lt;/span&gt;
&lt;a name=&quot;True-16&quot;&gt;&lt;/a&gt;    &lt;span class=&quot;n&quot;&gt;email&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=~&lt;/span&gt; &lt;span class=&quot;sr&quot;&gt;/\S+@\S+\.\S+/&lt;/span&gt;
&lt;a name=&quot;True-17&quot;&gt;&lt;/a&gt;  &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;a name=&quot;True-18&quot;&gt;&lt;/a&gt;&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here’s some CSS:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-css&quot; data-lang=&quot;css&quot;&gt;&lt;span class=&quot;nc&quot;&gt;.foobar&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;/* Named colors rule */&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;tomato&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here’s some JavaScript:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;isPresent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;require&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;is-present&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nx&quot;&gt;module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;exports&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;doStuff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;things&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;isPresent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;things&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;doOtherStuff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;things&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here’s some HTML:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;m0 p0 bg-blue white&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;h3&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;h1&amp;quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Hello, world!&lt;span class=&quot;nt&quot;&gt;&amp;lt;/h3&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h1 id=&quot;headings&quot;&gt;Headings!&lt;/h1&gt;

&lt;p&gt;They’re responsive, and well-proportioned (in &lt;code&gt;padding&lt;/code&gt;, &lt;code&gt;line-height&lt;/code&gt;, &lt;code&gt;margin&lt;/code&gt;, and &lt;code&gt;font-size&lt;/code&gt;).
They also heavily rely on the awesome utility, &lt;a href=&quot;http://www.basscss.com/&quot;&gt;BASSCSS&lt;/a&gt;.&lt;/p&gt;

&lt;h5 id=&quot;they-draw-the-perfect-amount-of-attention&quot;&gt;They draw the perfect amount of attention&lt;/h5&gt;

&lt;p&gt;This allows your content to have the proper informational and contextual hierarchy. Yay.&lt;/p&gt;

&lt;h3 id=&quot;there-are-lists-too&quot;&gt;There are lists, too&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Apples&lt;/li&gt;
  &lt;li&gt;Oranges&lt;/li&gt;
  &lt;li&gt;Potatoes&lt;/li&gt;
  &lt;li&gt;Milk&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Mow the lawn&lt;/li&gt;
  &lt;li&gt;Feed the dog&lt;/li&gt;
  &lt;li&gt;Dance&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;images-look-great-too&quot;&gt;Images look great, too&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://cloud.githubusercontent.com/assets/1424573/3378137/abac6d7c-fbe6-11e3-8e09-55745b6a8176.png&quot; alt=&quot;desk&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;img src=&quot;https://cloud.githubusercontent.com/assets/1424573/3378137/abac6d7c-fbe6-11e3-8e09-55745b6a8176.png&quot; alt=&quot;desk&quot; /&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;there-are-also-pretty-colors&quot;&gt;There are also pretty colors&lt;/h3&gt;

&lt;p&gt;Also the result of &lt;a href=&quot;http://www.basscss.com/&quot;&gt;BASSCSS&lt;/a&gt;, you can &lt;span class=&quot;bg-dark-gray white&quot;&gt;highlight&lt;/span&gt; certain components
of a &lt;span class=&quot;red&quot;&gt;post&lt;/span&gt; &lt;span class=&quot;mid-gray&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;green&quot;&gt;CSS&lt;/span&gt; &lt;span class=&quot;orange&quot;&gt;classes&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;I don’t recommend using blue, though. It looks like a &lt;span class=&quot;blue&quot;&gt;link&lt;/span&gt;.&lt;/p&gt;

&lt;h3 id=&quot;footnotes&quot;&gt;Footnotes!&lt;/h3&gt;

&lt;p&gt;Markdown footnotes are supported, and they look great! Simply put e.g. &lt;code&gt;[^1]&lt;/code&gt; where you want the footnote to appear,&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; and then add
the reference at the end of your markdown.&lt;/p&gt;

&lt;h3 id=&quot;stylish-blockquotes-included&quot;&gt;Stylish blockquotes included&lt;/h3&gt;

&lt;p&gt;You can use the markdown quote syntax, &lt;code&gt;&amp;gt;&lt;/code&gt; for simple quotes.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Suspendisse quis porta mauris.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;However, you need to inject html if you’d like a citation footer. I will be working on a way to
hopefully sidestep this inconvenience.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;
    Perfection is achieved, not when there is nothing more to add, but when there is nothing left to take away.
  &lt;/p&gt;
  &lt;footer&gt;&lt;cite title=&quot;Antoine de Saint-Exupéry&quot;&gt;Antoine de Saint-Exupéry&lt;/cite&gt;&lt;/footer&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;theres-more-being-added-all-the-time&quot;&gt;There’s more being added all the time&lt;/h3&gt;

&lt;p&gt;Checkout the &lt;a href=&quot;https://github.com/johnotander/pixyll&quot;&gt;Github repository&lt;/a&gt; to request,
or add, features.&lt;/p&gt;

&lt;p&gt;Happy writing.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Important information that may distract from the main text can go in footnotes. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Tue, 10 Jun 2014 05:31:19 -0700</pubDate>
        <link>http://offconvex.github.io/jekyll/pixyll/2014/06/10/mission-statement/</link>
        <guid isPermaLink="true">http://offconvex.github.io/jekyll/pixyll/2014/06/10/mission-statement/</guid>
      </item>
    
      <item>
        <title>So, What is Jekyll?</title>
        <description>&lt;p&gt;Jekyll is a tool for transforming your plain text into static websites and 
blogs. It is simple, static, and blog-aware. Jekyll uses the 
&lt;a href=&quot;http://docs.shopify.com/themes/liquid-basics&quot;&gt;Liquid&lt;/a&gt; templating
language and has builtin &lt;a href=&quot;http://daringfireball.net/projects/markdown/&quot;&gt;Markdown&lt;/a&gt;
and &lt;a href=&quot;http://en.wikipedia.org/wiki/Textile_(markup_language)&quot;&gt;Textile&lt;/a&gt; support.&lt;/p&gt;

&lt;p&gt;It also ties in nicely to &lt;a href=&quot;https://pages.github.com/&quot;&gt;Github Pages&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Learn more about Jekyll on their &lt;a href=&quot;http://jekyllrb.com/&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Jun 2014 05:32:18 -0700</pubDate>
        <link>http://offconvex.github.io/jekyll/pixyll/2014/06/09/so-what-is-jekyll/</link>
        <guid isPermaLink="true">http://offconvex.github.io/jekyll/pixyll/2014/06/09/so-what-is-jekyll/</guid>
      </item>
    
      <item>
        <title>Pixyll has Pagination</title>
        <description>&lt;p&gt;This is an empty post to illustrate the pagination component with Pixyll.&lt;/p&gt;
</description>
        <pubDate>Sun, 08 Jun 2014 04:21:29 -0700</pubDate>
        <link>http://offconvex.github.io/jekyll/pixyll/2014/06/08/pixyll-has-pagination/</link>
        <guid isPermaLink="true">http://offconvex.github.io/jekyll/pixyll/2014/06/08/pixyll-has-pagination/</guid>
      </item>
    
  </channel>
</rss>
